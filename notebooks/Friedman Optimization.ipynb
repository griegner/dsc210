{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db78b444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import datasets as ds\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "#import optimization as opt\n",
    "import sklearn.datasets as skds\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import robust_scale\n",
    "from sklearn import datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8b72b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "import torch\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcc211f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FRIEDMAN PROBLEM\n",
    "\n",
    "# MODELS / TRAINING\n",
    "class LinearRegression(torch.nn.Module):\n",
    "    \"\"\"Muliple linear regression as a perceptron.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    m_beta : int\n",
    "        Numbers of beta weights.\n",
    "    \"\"\"\n",
    "    def __init__(self, m_betas):\n",
    "        super(LinearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(m_betas, 1, bias=False)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "    \n",
    "class Friedman(torch.nn.Module):\n",
    "    \"\"\"Friedman regression as a perceptron.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    m_beta : int\n",
    "        Numbers of beta weights.\n",
    "    \"\"\"\n",
    "    def __init__(self,x):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.a = torch.nn.Parameter(torch.randn(()))\n",
    "        self.b = torch.nn.Parameter(torch.randn(()))\n",
    "        self.c = torch.nn.Parameter(torch.randn(()))\n",
    "        self.d = torch.nn.Parameter(torch.randn(()))\n",
    "        self.e = torch.nn.Parameter(torch.randn(()))\n",
    "        \n",
    "    def forward(self,x):\n",
    "            \n",
    "        return self.a*torch.sin(torch.pi*x[:,0]*x[:,1]) + self.b*(x[:,2]-self.c)**2 + self.d*(x[:,3]) + self.e* x[:,4]\n",
    "\n",
    "\n",
    "def train_model(X, y, method='sgd', lr=0.01, n_epochs=1000, opt_kwargs=None):\n",
    "    \"\"\"Train various gradient descent algorithms.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 2d tensor\n",
    "        Features.\n",
    "    y : 1d tensor\n",
    "        Target.\n",
    "    method : {'sgd', 'lbfgs', 'newton'}\n",
    "        Model to train.\n",
    "    lr : float, optional, default: 0.01\n",
    "        Learning rate (step size).\n",
    "    n_epochs : int, optional, default: 1000\n",
    "        Number of training iterations.\n",
    "    opt_kwargs : dict, optional, default: None\n",
    "        Optimizers kwargs to pass through to pytorch.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    betas : 1d array\n",
    "        Estimated beta parameters.\n",
    "    loss_hist : 1d array\n",
    "        Loss per step.\n",
    "    elapsed : float\n",
    "        Total time to train and run n_epochs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initalize model and loss function\n",
    "    #model = LinearRegression(len(X[0]))\n",
    "    model = Friedman(X)\n",
    "    loss_func = torch.nn.MSELoss()\n",
    "    loss_hist = np.zeros(n_epochs)\n",
    "    \n",
    "    # Select optimizer\n",
    "    opt_kwargs = {} if opt_kwargs is None else opt_kwargs\n",
    "    if method == 'sgd':\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, **opt_kwargs)\n",
    "    elif method == 'lbfgs':\n",
    "        optimizer = torch.optim.LBFGS(model.parameters(), lr=lr, **opt_kwargs)\n",
    "    elif method == 'newton':\n",
    "        betas = torch.rand(X.size(1), 1)\n",
    "    else:\n",
    "        raise ValueError('Undefined method.')\n",
    "\n",
    "    # Required for LBFGS\n",
    "    def closure():\n",
    "\n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Get predicted y\n",
    "        y_hat = model(X)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = loss_func(y_hat, y)\n",
    "        \n",
    "        # Update weights\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    # Required for Newton's method\n",
    "    def compute_loss(betas):\n",
    "        #y_hat = torch.matmul(X, betas)\n",
    "        y_hat = betas[0] * torch.sin(torch.pi*X[0]*X[1]) + betas[1] * (X[2] - betas[2])**2 +(betas[3]*X[3]) +(betas[4]*X[4]) # Friedman regression\n",
    "        return torch.nn.MSELoss()(y_hat, y)\n",
    "            \n",
    "    # Train\n",
    "    start = time.time()\n",
    "    for i in range(n_epochs):\n",
    "        \n",
    "        if method == 'sgd':\n",
    "            loss = closure()\n",
    "            optimizer.step()\n",
    "            loss_hist[i] = loss\n",
    "        elif method == 'lbfgs':\n",
    "            loss = optimizer.step(closure)\n",
    "            loss_hist[i] = loss\n",
    "        elif method == 'newton':\n",
    "\n",
    "            # Compute gradient and Hessian\n",
    "            grad = jacobian(compute_loss, betas)\n",
    "            \n",
    "            if X.size()[1] == 1: #check how many features\n",
    "                hess = torch.zeros((1,1)) # have to make a 1x1 tensor to be in unity with other cases when m > 1\n",
    "                # for some reason, hessian() output has no shape so we need to put it in the initialized 1x1 tensor\n",
    "                hess[0] =  hessian(compute_loss, betas).inverse().squeeze() \n",
    "            else:\n",
    "                hess = hessian(compute_loss, betas).squeeze().inverse()\n",
    "\n",
    "            # Step\n",
    "            betas = betas - torch.matmul(hess, grad)\n",
    "\n",
    "            loss_hist[i] = compute_loss(betas)\n",
    "\n",
    "    # Time\n",
    "    end = time.time()\n",
    "    elapsed = end-start\n",
    "    \n",
    "    if method != 'newton':\n",
    "        #betas = model.linear.weight.detach()[0]\n",
    "        betas = [param.data.item() for param in model.parameters()]\n",
    "    else:\n",
    "        betas = betas[:, 0]\n",
    "        \n",
    "    return betas, loss_hist, elapsed\n",
    "\n",
    "\n",
    "def test_train_model():\n",
    "    \"\"\"Test 1 beta and 10 beta cases.\"\"\"\n",
    "    \n",
    "    m = 100\n",
    "        \n",
    "    times = {}\n",
    "\n",
    "    X, beta, y = sim_lm(m * 10, m, seed=m)\n",
    "\n",
    "    for method in ['sgd', 'newton', 'lbfgs']:\n",
    "\n",
    "        beta_hat, loss_hist, elapsed = train_model(X, y, method=method)\n",
    "\n",
    "        times[method] = elapsed\n",
    "\n",
    "        # Check convergence\n",
    "        assert all(\n",
    "           beta_hat.numpy().round(2) == beta.numpy().round(2)\n",
    "        )\n",
    "\n",
    "    if m == 100:\n",
    "        assert times['sgd']  < times['lbfgs'] < times['newton']\n",
    "\n",
    "def save_data(n_sim,n_epochs,noise):\n",
    "    file = 'friedman_sim_data_'+'obs_'+str(n_sim)+'sims_'+str(n_epochs)+'_epochs_noise'+str(noise)+'.npz'\n",
    "    np.savez(file,beta_mse = beta_mse,elapsed_time = elapsed_time,loss_history = loss_history)\n",
    "# test_train_model() this is slow but passes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a6b94b",
   "metadata": {},
   "source": [
    "## Noise = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da74bad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51b497b291b74a2a96e73059a02e3227",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "b = np.array([10,20,0.5,10,5]) # DO NOT CHANGE, THESE ARE TRUE BETAS \n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_friedman1.html\n",
    "n_epochs = 250\n",
    "n_sim = 100\n",
    "noise = 0\n",
    "# Track loss, beta error, and elapsed time\n",
    "loss_history = {\n",
    "    'sgd': np.zeros((n_sim, n_epochs)),\n",
    "    'lbfgs': np.zeros((n_sim, n_epochs))\n",
    "}\n",
    "\n",
    "beta_mse = {\n",
    "    'sgd': np.zeros(n_sim),\n",
    "    'lbfgs': np.zeros(n_sim)\n",
    "}\n",
    "\n",
    "elapsed_time = {\n",
    "    'sgd': np.zeros(n_sim),\n",
    "    'lbfgs': np.zeros(n_sim)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "for i in tqdm(range(n_sim)): # 100 simulations take 6 minutes\n",
    "    X,y = datasets.make_friedman1(n_samples=100, n_features=5, noise=noise, random_state=i)\n",
    "    X_new = torch.tensor(X,requires_grad=True)\n",
    "    y_new = torch.tensor(y,requires_grad=True)\n",
    "    \n",
    "    b_hat_sgd, loss_hist_sgd, elapsed_sgd = train_model(X_new, y_new, method='sgd', n_epochs=n_epochs)\n",
    "    b_hat_lbfgs, loss_hist_lbfgs, elapsed_lbfgs = train_model(X_new, y_new, method='lbfgs', n_epochs=n_epochs)\n",
    "\n",
    "    beta_mse['sgd'][i] = float(((b - np.array(b_hat_sgd))**2).mean())\n",
    "    elapsed_time['sgd'][i] = elapsed_sgd\n",
    "    loss_history['sgd'][i] = loss_hist_sgd\n",
    "    \n",
    "    beta_mse['lbfgs'][i] = float(((b - np.array(b_hat_lbfgs))**2).mean())\n",
    "    elapsed_time['lbfgs'][i] = elapsed_lbfgs\n",
    "    loss_history['lbfgs'][i] = loss_hist_lbfgs\n",
    "save_data(n_sim,n_epochs,noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94a2f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(12, 4), ncols=2)\n",
    "def get_finite_vals(array):\n",
    "    ii = np.isfinite(array)\n",
    "    return(array[ii])\n",
    "    \n",
    "axes[0].violinplot(get_finite_vals(beta_mse['sgd']))\n",
    "axes[0].violinplot(get_finite_vals(beta_mse['lbfgs']), positions=[1.5])\n",
    "    \n",
    "axes[1].violinplot(get_finite_vals(elapsed_time['sgd']))\n",
    "axes[1].violinplot(get_finite_vals(elapsed_time['lbfgs']), positions=[1.5])\n",
    "    \n",
    "axes[0].set_xticks([1, 1.5], ['SGD', 'LBFGS'])\n",
    "axes[1].set_xticks([1, 1.5], ['SGD', 'LBFGS'])\n",
    "    \n",
    "axes[0].set_title(f'MSE: 0 Noise')\n",
    "axes[1].set_title(f'SGD vs LBFGS Time Elapsed : 0 Noise')\n",
    "    \n",
    "axes[0].set_ylabel('MSE')\n",
    "axes[1].set_ylabel('Time (s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71871023",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_mse_df = pd.DataFrame(beta_mse['sgd'])\n",
    "lbfgs_mse_df = pd.DataFrame(beta_mse['lbfgs'])\n",
    "n = 2\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 18))\n",
    "sgd_mse_df.hist(ax = ax[0],bins = 20)\n",
    "ax[0].set_title('Distribution of SGD MSE (Friedman, 0 noise) across 100 Simulations')\n",
    "ax[0].set_xlabel('MSE')\n",
    "ax[0].set_ylabel('Count')\n",
    "lbfgs_mse_df.hist(ax = ax[1], bins = 20, color = 'orange')\n",
    "ax[1].set_title('Distribution of LBFGS MSE (Friedman, 0 noise) across 100 Simulations')\n",
    "ax[1].set_xlabel('MSE')\n",
    "ax[1].set_ylabel('Count')\n",
    "\n",
    "\n",
    "five_num_summary_mse = pd.concat([sgd_mse_df.describe(),lbfgs_mse_df.describe()],axis = 1)\n",
    "five_num_summary_mse.columns = ['SGD','LBFGS']\n",
    "fname = 'friedman_sim_data_'+str(n_sim)+'sims_'+str(n_epochs)+'epochs_'+str(noise)+'noise_mse5summ.xlsx'\n",
    "five_num_summary_mse.to_excel(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "677ccfca",
   "metadata": {},
   "source": [
    "## Noise = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08ed230",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array([10,20,0.5,10,5]) # DO NOT CHANGE, THESE ARE TRUE BETAS \n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_friedman1.html\n",
    "n_epochs = 250\n",
    "n_sim = 100\n",
    "noise = 50\n",
    "# Track loss, beta error, and elapsed time\n",
    "loss_history = {\n",
    "    'sgd': np.zeros((n_sim, n_epochs)),\n",
    "    'lbfgs': np.zeros((n_sim, n_epochs))\n",
    "}\n",
    "\n",
    "beta_mse = {\n",
    "    'sgd': np.zeros(n_sim),\n",
    "    'lbfgs': np.zeros(n_sim)\n",
    "}\n",
    "\n",
    "elapsed_time = {\n",
    "    'sgd': np.zeros(n_sim),\n",
    "    'lbfgs': np.zeros(n_sim)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "for i in tqdm(range(n_sim)): # 100 simulations take 6 minutes\n",
    "    X,y = datasets.make_friedman1(n_samples=100, n_features=5, noise=noise, random_state=i)\n",
    "    X_new = torch.tensor(X,requires_grad=True)\n",
    "    y_new = torch.tensor(y,requires_grad=True)\n",
    "    \n",
    "    b_hat_sgd, loss_hist_sgd, elapsed_sgd = train_model(X_new, y_new, method='sgd', n_epochs=n_epochs)\n",
    "    b_hat_lbfgs, loss_hist_lbfgs, elapsed_lbfgs = train_model(X_new, y_new, method='lbfgs', n_epochs=n_epochs)\n",
    "\n",
    "    beta_mse['sgd'][i] = float(((b - np.array(b_hat_sgd))**2).mean())\n",
    "    elapsed_time['sgd'][i] = elapsed_sgd\n",
    "    loss_history['sgd'][i] = loss_hist_sgd\n",
    "    \n",
    "    beta_mse['lbfgs'][i] = float(((b - np.array(b_hat_lbfgs))**2).mean())\n",
    "    elapsed_time['lbfgs'][i] = elapsed_lbfgs\n",
    "    loss_history['lbfgs'][i] = loss_hist_lbfgs\n",
    "save_data(n_sim,n_epochs,noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dd5d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(12, 4), ncols=2)\n",
    "def get_finite_vals(array):\n",
    "    ii = np.isfinite(array)\n",
    "    return(array[ii])\n",
    "    \n",
    "axes[0].violinplot(get_finite_vals(beta_mse['sgd']))\n",
    "axes[0].violinplot(get_finite_vals(beta_mse['lbfgs']), positions=[1.5])\n",
    "    \n",
    "axes[1].violinplot(get_finite_vals(elapsed_time['sgd']))\n",
    "axes[1].violinplot(get_finite_vals(elapsed_time['lbfgs']), positions=[1.5])\n",
    "    \n",
    "axes[0].set_xticks([1, 1.5], ['SGD', 'LBFGS'])\n",
    "axes[1].set_xticks([1, 1.5], ['SGD', 'LBFGS'])\n",
    "    \n",
    "axes[0].set_title(f'MSE: '+str(noise)+' Noise')\n",
    "axes[1].set_title(f'SGD vs LBFGS Time Elapsed : '+str(noise)+' Noise')\n",
    "    \n",
    "axes[0].set_ylabel('MSE')\n",
    "axes[1].set_ylabel('Time (s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc73aaea",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_mse_df = pd.DataFrame(beta_mse['sgd'])\n",
    "lbfgs_mse_df = pd.DataFrame(beta_mse['lbfgs'])\n",
    "n = 2\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 18))\n",
    "sgd_mse_df.hist(ax = ax[0],bins = 20)\n",
    "ax[0].set_title('Distribution of SGD MSE (Friedman, '+str(noise)+' noise) across 100 Simulations')\n",
    "ax[0].set_xlabel('MSE')\n",
    "ax[0].set_ylabel('Count')\n",
    "lbfgs_mse_df.hist(ax = ax[1], bins = 20, color = 'orange')\n",
    "ax[1].set_title('Distribution of LBFGS MSE (Friedman, '+str(noise)+' noise) across 100 Simulations')\n",
    "ax[1].set_xlabel('MSE')\n",
    "ax[1].set_ylabel('Count')\n",
    "\n",
    "five_num_summary_mse = pd.concat([sgd_mse_df.describe(),lbfgs_mse_df.describe()],axis = 1)\n",
    "five_num_summary_mse.columns = ['SGD','LBFGS']\n",
    "fname = 'friedman_sim_data_'+str(n_sim)+'sims_'+str(n_epochs)+'epochs_'+str(noise)+'noise_mse5summ.xlsx'\n",
    "five_num_summary_mse.to_excel(fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab05c954",
   "metadata": {},
   "source": [
    "## 100 noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8ba8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = np.array([10,20,0.5,10,5]) # DO NOT CHANGE, THESE ARE TRUE BETAS \n",
    "#https://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_friedman1.html\n",
    "n_epochs = 250\n",
    "n_sim = 100\n",
    "noise = 100\n",
    "# Track loss, beta error, and elapsed time\n",
    "loss_history = {\n",
    "    'sgd': np.zeros((n_sim, n_epochs)),\n",
    "    'lbfgs': np.zeros((n_sim, n_epochs))\n",
    "}\n",
    "\n",
    "beta_mse = {\n",
    "    'sgd': np.zeros(n_sim),\n",
    "    'lbfgs': np.zeros(n_sim)\n",
    "}\n",
    "\n",
    "elapsed_time = {\n",
    "    'sgd': np.zeros(n_sim),\n",
    "    'lbfgs': np.zeros(n_sim)\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "for i in tqdm(range(n_sim)): # 100 simulations take 6 minutes\n",
    "    X,y = datasets.make_friedman1(n_samples=100, n_features=5, noise=noise, random_state=i)\n",
    "    X_new = torch.tensor(X,requires_grad=True)\n",
    "    y_new = torch.tensor(y,requires_grad=True)\n",
    "    \n",
    "    b_hat_sgd, loss_hist_sgd, elapsed_sgd = train_model(X_new, y_new, method='sgd', n_epochs=n_epochs)\n",
    "    b_hat_lbfgs, loss_hist_lbfgs, elapsed_lbfgs = train_model(X_new, y_new, method='lbfgs', n_epochs=n_epochs)\n",
    "\n",
    "    beta_mse['sgd'][i] = float(((b - np.array(b_hat_sgd))**2).mean())\n",
    "    elapsed_time['sgd'][i] = elapsed_sgd\n",
    "    loss_history['sgd'][i] = loss_hist_sgd\n",
    "    \n",
    "    beta_mse['lbfgs'][i] = float(((b - np.array(b_hat_lbfgs))**2).mean())\n",
    "    elapsed_time['lbfgs'][i] = elapsed_lbfgs\n",
    "    loss_history['lbfgs'][i] = loss_hist_lbfgs\n",
    "save_data(n_sim,n_epochs,noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccfb48fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(12, 4), ncols=2)\n",
    "def get_finite_vals(array):\n",
    "    ii = np.isfinite(array)\n",
    "    return(array[ii])\n",
    "    \n",
    "axes[0].violinplot(get_finite_vals(beta_mse['sgd']))\n",
    "axes[0].violinplot(get_finite_vals(beta_mse['lbfgs']), positions=[1.5])\n",
    "    \n",
    "axes[1].violinplot(get_finite_vals(elapsed_time['sgd']))\n",
    "axes[1].violinplot(get_finite_vals(elapsed_time['lbfgs']), positions=[1.5])\n",
    "    \n",
    "axes[0].set_xticks([1, 1.5], ['SGD', 'LBFGS'])\n",
    "axes[1].set_xticks([1, 1.5], ['SGD', 'LBFGS'])\n",
    "    \n",
    "axes[0].set_title(f'MSE: '+str(noise)+' Noise')\n",
    "axes[1].set_title(f'SGD vs LBFGS Time Elapsed : '+str(noise)+' Noise')\n",
    "    \n",
    "axes[0].set_ylabel('MSE')\n",
    "axes[1].set_ylabel('Time (s)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb3be1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_mse_df = pd.DataFrame(beta_mse['sgd'])\n",
    "lbfgs_mse_df = pd.DataFrame(beta_mse['lbfgs'])\n",
    "n = 2\n",
    "fig, ax = plt.subplots(1, 2, figsize=(20, 18))\n",
    "sgd_mse_df.hist(ax = ax[0],bins = 20)\n",
    "ax[0].set_title('Distribution of SGD MSE (Friedman, '+str(noise)+' noise) across 100 Simulations')\n",
    "ax[0].set_xlabel('MSE')\n",
    "ax[0].set_ylabel('Count')\n",
    "lbfgs_mse_df.hist(ax = ax[1], bins = 20, color = 'orange')\n",
    "ax[1].set_title('Distribution of LBFGS MSE (Friedman, '+str(noise)+' noise) across 100 Simulations')\n",
    "ax[1].set_xlabel('MSE')\n",
    "ax[1].set_ylabel('Count')\n",
    "\n",
    "five_num_summary_mse = pd.concat([sgd_mse_df.describe(),lbfgs_mse_df.describe()],axis = 1)\n",
    "five_num_summary_mse.columns = ['SGD','LBFGS']\n",
    "fname = 'friedman_sim_data_'+str(n_sim)+'sims_'+str(n_epochs)+'epochs_'+str(noise)+'noise_mse5summ.xlsx'\n",
    "five_num_summary_mse.to_excel(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5f088d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235c9647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "024ad45c",
   "metadata": {},
   "source": [
    "The suprising thing here is that with hand calculations, you will get the wrong result. The LGBFS model is able to perform even with the noise and get the true $\\beta$. The SGD algorithm is not able to discern it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ab0839",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = datasets.make_friedman1(n_samples=100, n_features=5, noise=100, random_state=1)\n",
    "X_new = torch.tensor(X,requires_grad=True)\n",
    "y_new = torch.tensor(y,requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0a6718",
   "metadata": {},
   "outputs": [],
   "source": [
    "b[0]*torch.sin(torch.pi*X_new[:,0]*X_new[:,1]) + b[1]*(X_new[:,2]-b[2])**2 + (b[3]*(X_new[:,3])) + (b[4]* X_new[:,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4353a280",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a25a685",
   "metadata": {},
   "source": [
    "## Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2932869a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y = datasets.make_friedman1(n_samples=100, n_features=5, noise=0, random_state=1)\n",
    "X_new = torch.tensor(X,requires_grad=True)\n",
    "y_new = torch.tensor(y,requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5addcbc6",
   "metadata": {},
   "source": [
    "With 0 noise, it matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e69f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "b[0]*torch.sin(torch.pi*X_new[:,0]*X_new[:,1]) + b[1]*(X_new[:,2]-b[2])**2 + (b[3]*(X_new[:,3])) + (b[4]* X_new[:,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd8a990",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_new"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
