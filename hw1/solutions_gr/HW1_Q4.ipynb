{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# 4. Google Pagerank Algorithm (10 points)\n","\n","**Keywords**: Pagerank, Power Method\n","\n","**About the dataset**: \\\n","*DBpedia* (from \"DB\" for \"database\") is a project aiming to extract structured content from the information created in the Wikipedia project. This structured information is made available on the World Wide Web. DBpedia allows users to semantically query relationships and properties of Wikipedia resources, including links to other related datasets. for more info, see: https://en.wikipedia.org/wiki/DBpedia. \\\n","We will download two files from the data respository:\n","* The first file -- **redirects_en.nt.bz2** -- contains redirects link for a page. Let A redirect to B and B redirect to C. Then we will replace article A by article C wherever needed.\n","* The second file -- **page_links_en.nt.bz2** -- contains pagelinks which are links within an article to other wiki article.\n","\n","Note that the data is both files is a list of lines which can be split into 4 parts:\n","* The link to first article.\n","* Whether it is a redirect, or a pagelink.\n","* The link to second article.\n","* A fullstop.\n","\n","#### <font color=\"red\">Note:</font> Any line which cannot be split into 4 parts is skipped from consideration.\n","\n","**Agenda**:\n","* In this programming challenge, you will be implementing the [*google pagerank algorithm*](https://towardsdatascience.com/pagerank-algorithm-fully-explained-dc794184b4af) to determine the most important articles.\n","* This will be done by computing the principal eigenvector of the article-article graph adjacency matrix.\n","* In this challenge, you will be applying the *power method* to extract the principal eigenvector from the adjacency matrix. \n","* Using the computed eigenvector, we can assign each article a eigenvector-centrality score. Then we can determine the most important articles.\n","\n","**Environment**:\n","Ensure following libraries are installed\n","- sklearn\n","- numpy\n","\n","Also ensure that you have around **4 GB** of memory.\n","\n","**Note:**\n","* Run all the cells in order.\n","* **Do not edit** the cells marked with !!DO NOT EDIT!!\n","* Only **add your code** to cells marked with !!!! YOUR CODE HERE !!!!\n","* Do not change variable names, and use the names which are suggested."],"metadata":{"id":"lJCEyyf3IlRw"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"b2ivsajX4yuy"}},{"cell_type":"code","source":["# !! DO NOT EDIT !!\n","# imports\n","import pickle\n","from bz2 import BZ2File\n","import bz2\n","import os\n","from datetime import datetime\n","import pprint\n","from time import time\n","import numpy as np\n","from urllib.request import urlopen\n","import scipy.sparse as sparse\n","pp = pprint.PrettyPrinter(indent=4)"],"metadata":{"id":"LfVmAEtTfNXf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !! DO NOT EDIT !!\n","# download the dataset and store files in local\n","\n","# dbpedia download urls\n","redirects_url = \"http://downloads.dbpedia.org/3.5.1/en/redirects_en.nt.bz2\"\n","page_links_url = \"http://downloads.dbpedia.org/3.5.1/en/page_links_en.nt.bz2\"\n","\n","# extarct the file-names from the urls. Needed to load the files later\n","redirects_filename = redirects_url.rsplit(\"/\", 1)[1] # redirects_en.nt.bz2 ~ 59MB\n","page_links_filename = page_links_url.rsplit(\"/\", 1)[1] # page_links_en.nt.bz2 ~ 850MB\n","\n","resources = [\n","    (redirects_url, redirects_filename),\n","    (page_links_url, page_links_filename),\n","]\n","\n","# download the files\n","# this will take some time\n","for url, filename in resources:\n","    if not os.path.exists(filename):\n","        print(\"Downloading data from '%s', please wait...\" % url)\n","        opener = urlopen(url)\n","        open(filename, \"wb\").write(opener.read())\n","        print()"],"metadata":{"id":"OARGSLGW50J8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1650400206325,"user_tz":420,"elapsed":43635,"user":{"displayName":"Yash Khandelwal","userId":"13446769462813321665"}},"outputId":"2c18e418-b04a-4a38-ef6e-f6384c944e5f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from 'http://downloads.dbpedia.org/3.5.1/en/redirects_en.nt.bz2', please wait...\n","\n","Downloading data from 'http://downloads.dbpedia.org/3.5.1/en/page_links_en.nt.bz2', please wait...\n","\n"]}]},{"cell_type":"code","source":["# !! DO NOT EDIT !!\n","# load the data from the downloaded files\n","# this may take some time\n","\n","#read redirects file\n","redirects_file = bz2.open(redirects_filename, mode='rt')\n","redirects_data = redirects_file.readlines()\n","redirects_file.close()\n","\n","# pagelinks data has 119M entries\n","# We will only consider the first 5M for this question\n","pagelinks_file = bz2.open(page_links_filename, mode='rt')\n","pagelinks_data = [next(pagelinks_file) for x in range(5000000)] \n","pagelinks_file.close()"],"metadata":{"id":"7Rvb3xDT51Ky"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !! DO NOT EDIT !!\n","# look at the size of the data and some examples\n","print ('The number of entries in redirects:', len(redirects_data))\n","print ('A couple of examples from redirects:')\n","print (redirects_data[10000:10002])\n","print ('\\n')\n","\n","print ('The number of entries in pagelinks:', len(pagelinks_data))\n","print ('A couple of examples from pagelinks:')\n","print (pagelinks_data[100000:100002])"],"metadata":{"id":"ZGcZQ_SEhuEv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### <font color=\"red\">Note:</font> It is worth noting here that each article is uniquely represented by its URL, or rather, the last segment of its URL"],"metadata":{"id":"5JLgMS37hw_O"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"3A_oqICv4tYn"}},{"cell_type":"markdown","source":["### **(a)** Define a function `get_article_name` which takes as input the URL string, and extracts the last segment of the URL, which we can call as article name. (1 point)"],"metadata":{"id":"nEj1Gbc2JDn8"}},{"cell_type":"code","source":["#######\n","# !!!! YOUR CODE HERE !!!!\n","\n","#######"],"metadata":{"id":"2Olrh2zv6Hv1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !! DO NOT EDIT !!\n","# some unit tests to validate your solution\n","assert get_article_name('<http://dbpedia.org/resource/Pope_Alexander_V>') == 'Pope_Alexander_V'\n","assert get_article_name('<http://dbpedia.org/resource/Jean-Paul_Sartre>') == 'Jean-Paul_Sartre'"],"metadata":{"id":"2iwIPZp4h7OK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n"],"metadata":{"id":"hVxitX-eiGhX"}},{"cell_type":"markdown","source":["### **(b)** Define a function `resolve_redirects` which takes as input a list of redirect lines, and returns a map between the initial and the resolved redirect page. (2 points)\n","\n","e.g.: input = \\\n","[\n","'\\<http://dbpedia.org/resource/A> \\<http://dbpedia.org/property/redirect> \\<http://dbpedia.org/resource/B> .\\n',\\\n","'\\<http://dbpedia.org/resource/B> \\<http://dbpedia.org/property/redirect> \\<http://dbpedia.org/resource/C> .\\n',\\\n","'\\<http://dbpedia.org/resource/C> \\<http://dbpedia.org/property/redirect> \\<http://dbpedia.org/resource/D> .\\n',\\\n","'\\<http://dbpedia.org/resource/X> \\<http://dbpedia.org/property/redirect> \\<http://dbpedia.org/resource/Z> .\\n'\n","]\n","\n","output = {'A': 'D', 'B': 'D', 'C': 'D', 'X': 'Z'}\n","\n","#### <font color=\"red\">Note:</font> Remember to ignore malformed lines which are those which do not split in 4 parts."],"metadata":{"id":"FGVnitK0iASM"}},{"cell_type":"code","source":["#######\n","# !!!! YOUR CODE HERE !!!!\n","\n","#######"],"metadata":{"id":"6TyhO_2k3zhd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !! DO NOT EDIT !!\n","# some unit tests to validate your solution\n","test_input = ['<http://dbpedia.org/resource/A> <http://dbpedia.org/property/redirect> <http://dbpedia.org/resource/B> .\\n', \n","              '<http://dbpedia.org/resource/B> <http://dbpedia.org/property/redirect> <http://dbpedia.org/resource/C> .\\n', \n","              '<http://dbpedia.org/resource/C> <http://dbpedia.org/property/redirect> <http://dbpedia.org/resource/D> .\\n', \n","              '<http://dbpedia.org/resource/X> <http://dbpedia.org/property/redirect> <http://dbpedia.org/resource/Z> .\\n']\n","\n","test_output = {'A': 'D', 'B': 'D', 'C': 'D', 'X': 'Z'}\n","\n","assert resolve_redirects(test_input) == test_output"],"metadata":{"id":"IWRohtGmiKB0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"HU0xRjui426w"}},{"cell_type":"markdown","source":["### **(c)** Create article-article adjacency matrix. \n","### Let the number of articles $n$. The adjacency matrix should have a value $A[i][j]=1$ if there is a link from $i$ to $j$. Note that the matrix may not be symmetric. This matrix would have rows as source, and columns as destinations. However, for further sections, we need it the other way round. Therefore, return $A^\\top$ matrix. \n","### Create a function `make_adjacency_matrix` that takes as input the resolved redirect map from part (b), and the list from `pagelinks_data`.\n","### Return a tuple of `(index_map, A')`, where `index_map` is a map of each article to a unique number between $0$ and $n-1$, also its unique numerical id. `A` is the adjacency matrix in [scipy.sparse.csr_matrix](https://docs.scipy.org/doc/scipy/reference/generated/scipy.sparse.csr_matrix.html) format. `A'` is the transpose of matrix `A`. (2 points)\n","\n","\n","#### <font color=\"red\">Note:</font> Take care that if A redirects to D and X redirects to Y, and there is a pagelink entry from A to X, then the resolved pagelink entry should be D to Y."],"metadata":{"id":"kKI0kyPGJdgh"}},{"cell_type":"code","source":["#######\n","# !!!! YOUR CODE HERE !!!!\n","\n","#######"],"metadata":{"id":"C9tE3opr4GOF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !! DO NOT EDIT !!\n","# some unit tests to validate your solution\n","test_redirects = {'A': 'D', 'B': 'D', 'C': 'D', 'X': 'Z', 'K':'L', 'M':'N'}\n","test_pagelinks_data = ['<http://dbpedia.org/resource/A> <http://dbpedia.org/property/wikilink> <http://dbpedia.org/resource/X> .\\n', '<http://dbpedia.org/resource/L> <http://dbpedia.org/property/wikilink> <http://dbpedia.org/resource/N> .\\n', '<http://dbpedia.org/resource/P> <http://dbpedia.org/property/wikilink> <http://dbpedia.org/resource/Q> .\\n']\n","\n","test_output_index_map = {'D': 0, 'Z': 1, 'L': 2, 'N': 3, 'P': 4, 'Q': 5}\n","test_output_adjacency_matrix = np.array([[0., 1., 0., 0., 0., 0.],\n","                                           [0., 0., 0., 0., 0., 0.],\n","                                           [0., 0., 0., 1., 0., 0.],\n","                                           [0., 0., 0., 0., 0., 0.],\n","                                           [0., 0., 0., 0., 0., 1.],\n","                                           [0., 0., 0., 0., 0., 0.]])\n","\n","output_index_map, output_A = make_adjacency_matrix(test_redirects, test_pagelinks_data) \n","\n","assert output_index_map == test_output_index_map\n","np.testing.assert_array_equal(output_A.toarray(), test_output_adjacency_matrix.T)"],"metadata":{"id":"H9EbL700tM83"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"TkATsaLd44Sz"}},{"cell_type":"markdown","source":["### **(d)** Apply the above functions on the dataset to create adjacency matrix $A$ and other relevant maps as directed below. Then apply `SVD` from sklearn on the adjacency matrix to determine principal singular vectors. (2 points)"],"metadata":{"id":"dADe7PA64Ups"}},{"cell_type":"code","source":["#######\n","# !!!! YOUR CODE HERE !!!!\n","# 1. with redirects_data as input, use the resolve_redirects function to generate the redirects_map\n","# redirects_map = ________\n","\n","# 2. with redirects map from previous step pagelinks_data as inputs, use the make_adjacency_matrix to generate index_map and adjacency_matrix\n","# index_map, X = ________\n","\n","# 3. using index_map, create a reverse_index_map, which has the article name as key, and its index as value\n","# reverse_index_map = ________\n","\n","#######"],"metadata":{"id":"SLWYY6b14UGB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !! DO NOT EDIT !!\n","# Now we will save the csr matrix, index_map and reverse_index_map in pickle files\n","# so that we do not have to recompute steps (a)-(d) next time we load the program\n","# (Note: beneficial only when working on local machine, as colab session times out)\n","PATH='./'\n","pickle.dump(X, open(PATH+'X.pkl', 'wb'))\n","pickle.dump(index_map, open(PATH+'index_map.pkl', 'wb'))\n","pickle.dump(reverse_index_map, open(PATH+'reverse_index_map.pkl', 'wb'))\n","\n","# free up RAM\n","del(redirects_data, pagelinks_data)"],"metadata":{"id":"vve3BcBJyWVj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["! ---------- Checkpoint ----------- !"],"metadata":{"id":"6_UZ3wyQx7D0"}},{"cell_type":"code","source":["# !! DO NOT EDIT !!\n","# Load the data from here\n","PATH='./'\n","X = pickle.load(open(PATH+'X.pkl', 'rb'))\n","index_map = pickle.load(open(PATH+'index_map.pkl', 'rb'))\n","reverse_index_map =  pickle.load(open(PATH+'reverse_index_map.pkl', 'rb'))"],"metadata":{"id":"CUFjziZoxUTX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Apply ```randomized_svd``` from sklearn on the adjacency matrix. Extract top 5 components and run for 3 iterations."],"metadata":{"id":"_ZPZghiu5iIY"}},{"cell_type":"code","source":["#######\n","# !!!! YOUR CODE HERE !!!!\n","# U, s, V = __________\n","\n","#######"],"metadata":{"id":"8pU8CY5v5C38"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !! DO NOT EDIT !!\n","# now, we print the names of the wikipedia related strongest components of the\n","# principal singular vector which should be similar to the highest eigenvector\n","print(\"Top wikipedia pages according to principal singular vectors\")\n","pp.pprint([reverse_index_map[i] for i in np.abs(U.T[0]).argsort()[-10:]])\n","pp.pprint([reverse_index_map[i] for i in np.abs(V[0]).argsort()[-10:]])"],"metadata":{"id":"07VFJzflIt2p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"3uBMcqfuKofd"}},{"cell_type":"markdown","source":["### **(e)** The pagerank algorithm \n","### In this final section we will implementing the google pagerank algorithm by computing principal eigenvector using power iteration method. (3 points)\n","\n","### To start with the power iteration method, we first need to make the matrix $X$ obtained in (d) *column stochastic*. A column stochastic matrix is a matrix in which each element represents a probability and the sum each column adds up to 1. Recall that $X$ is a matrix where the rows represent the destination and columns represents the source. The probability of visiting any destination from the source $s$ is $1/k$, where $k$ is the total number of outgoing links from $s$. Use this information to make the matrix column stochastic.  \n"],"metadata":{"id":"Yc-kNfb_6cTe"}},{"cell_type":"code","source":["#######\n","# !!!! YOUR CODE HERE !!!!\n","\n","#######"],"metadata":{"id":"42X9k-ow6bqY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### *Dangling Nodes*: There may exisit some pages which have no outgoing links. These are called as dangling nodes. If a random surfer just follows outgoing page links, then such a person can never leave a dangling node. We cannot just skip such a node, as there may be many pages pointing to this page, and could therefore be important. \n","### To solve this problem, we introduce **teleportation** which says that a random surfer will visit an outgoing link with $\\beta$ probability and can randomly jump to some other page with a $(1-\\beta)/n$ probability (like through bookmarks, directly going through URL, etc.). Here $n$ is the total number of pages under consideration, and $\\beta$ is called the damping factor. So now, the modified transition matrix is:\n","### $R = \\beta X + \\frac{(1-\\beta)}{n} I_{n\\times n}$\n","\n","### where $X$ is the column stochastic matrix from previous step, and $I_{n\\times n}$ is a $n\\times n$ identity matrix.\n","\n","### Using the transition matrix $R$, use the power iteration method to solve for the principal eigenvector $\\mathbf{p}_{n\\times 1}$. Start with an initial guess of $\\mathbf{p}_{n\\times 1}=[\\frac{1}{n}, \\frac{1}{n}, ..., \\frac{1}{n}]$, which intuitively represents that a random surfer can start at any page with a $\\frac{1}{n}$ probability. Use a damping factor of $0.85$, and perform a maximum of 100 iterations.\n","\n","### Report the top 10 page names which correspond to the top 10 scores (magnitudes) in the principal eigenvector.\n","\n","\n"],"metadata":{"id":"BaiGbHgO-1xI"}},{"cell_type":"code","source":["#######\n","# !!!! YOUR CODE HERE !!!!\n","\n","#######"],"metadata":{"id":"cxLCTSisDUBJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"mNvW5iW4yJ2y"}}]}