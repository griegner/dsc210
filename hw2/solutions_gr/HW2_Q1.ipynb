{"cells":[{"cell_type":"markdown","metadata":{"id":"hjRM13EAuL3g"},"source":["# HW2 - Q1: Least Squares Regression (30 points)\n","Notes:\n","* Question (a) needs to be typewritten.\n","* Questions (b), (c), and (d) need to be programmed.\n","* Important:\n","  * Write all the steps of the solution. \n","  * Use proper LATEX formatting and notation for all mathematical equations, vectors, and matrices. \n","* For programming solution:\n","  * Properly add comments to your code.\n","\n","#### <font color=\"red\">A note about notation:</font>\n","The notations in this homework are slightly different from the lecture notes. In lecture, we use notation for data as: $(t_i, y_i)$ with regressor $\\hat{y}=x^\\top t$, $x$ is a vector of unknown coefficients and solve $Ax=b$.\\\n","In this homework, the notation that we use for data is: $(x_i, y_i)$ with regressor $\\hat{y}=\\beta^\\top x$ and $\\beta$ is a vector of unknown coefficients to be solved.\n"]},{"cell_type":"markdown","metadata":{"id":"ZUJgYttwjmpK"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"XUycEdJg6hzY"},"source":["### **(a)** Consider a dataset with $m$ datapoints: $(x_i, y_i), i=1,...,m$. Perform the multivariate calculus derivation of the least squares regression formula for an estimation function $ùë¶ÃÇ(ùë•)=ùëéùë•^2+ùëèùë•+ùëê$, where $a,b, \\text{and } c$ are the scalar parameters. (6 points)"]},{"cell_type":"markdown","metadata":{"id":"Fthrg3347BEr"},"source":["#### <font color=\"red\">Your answer here:</font>"]},{"cell_type":"markdown","metadata":{},"source":["$$\\hat{y}(x) = ax^2 + bx + c  \\tag{1}$$\n","\n","To solve the unknown parameters $a,b, \\text{and } c$ of the function **(1)** that best fits the data $\\{(x_i, y_i)\\}_{i=1}^m$, we define the least squares loss function as:  \n","\n","$$\n","\\begin{aligned}\n","\n","\\phi(a,b,c) &= \\sum_{i=1}^m (y_i - \\overbrace{(ax_i^2 + bx_i +c)}^{\\hat{y_i}})^2 \\\\\n","&= \\sum_{i=1}^m (y_i - ax_i^2 - bx_i - c)^2  \\tag{2}\n","\n","\\end{aligned}\n","$$\n","\n","Find the parameters $a,b, \\text{and } c$ that minimize **(2)** s.t.  \n","\n","$$\\frac{\\partial \\phi}{\\partial a} = \\frac{\\partial \\phi}{\\partial b} = \\frac{\\partial \\phi}{\\partial c} = 0  \\tag{3}$$  \n","\n","$\\nabla \\phi(a,b,c) = 0$ yields:  \n","\n","$$\n","\\begin{aligned}\n","\\frac{\\partial \\phi}{\\partial a} &= -2 \\sum_{i=1}^m x_i^2(y_i - ax_i^2 -bx_1 -c) = 0 \\\\\n","&= a\\sum_{i=1}^mx_i^4 + b\\sum_{i=1}^mx_i^3 + c\\sum_{i=1}^mx_i^2 = \\sum_{i=1}^mx_i^2y_i \\\\\n","\n","\\\\\\\\\n","\n","\\frac{\\partial \\phi}{\\partial b} &= -2 \\sum_{i=1}^m x_i(y_i - ax_i^2 -bx_1 -c) = 0 \\\\\n","&= a\\sum_{i=1}^mx_i^3 + b\\sum_{i=1}^mx_i^2 + c\\sum_{i=1}^mx_i = \\sum_{i=1}^mx_iy_i \\\\\n","\n","\\\\\\\\\n","\n","\\frac{\\partial \\phi}{\\partial c} &= -2 \\sum_{i=1}^m (y_i - ax_i^2 -bx_1 -c) = 0 \\\\\n","&= a\\sum_{i=1}^mx_i^2 + b\\sum_{i=1}^mx_i + cm = \\sum_{i=1}^my_i  \n","\n","\\tag{4}\n","\n","\\end{aligned}\n","$$\n","\n","written in matrix form:  \n","\n","$$\n","\\begin{aligned}\n","\n","\\begin{bmatrix}\n","\\displaystyle\\sum_{i=1}^m x_i^4 & \\displaystyle\\sum_{i=1}^m x_i^3 & \\displaystyle\\sum_{i=1}^m x_i^2 \\\\\n","\\displaystyle\\sum_{i=1}^m x_i^3 & \\displaystyle\\sum_{i=1}^m x_i^2 & \\displaystyle\\sum_{i=1}^m x_i \\\\\n","\\displaystyle\\sum_{i=1}^m x_i^2 & \\displaystyle\\sum_{i=1}^m x_i & m \\\\\n","\\end{bmatrix}\n","\n","\\begin{bmatrix}\n","a \\\\ b \\\\ c\n","\\end{bmatrix}\n","\n","&=\n","\n","\\begin{bmatrix}\n","\\displaystyle\\sum_{i=1}^m x_i^2y_i\\\\\n","\\displaystyle\\sum_{i=1}^m x_iy_i\\\\\n","\\displaystyle\\sum_{i=1}^m y_i\\\\\n","\\end{bmatrix}\n","\n","\\\\\\\\\n","\n","\\begin{bmatrix}\n","a \\\\ b \\\\ c\n","\\end{bmatrix}\n","\n","&=\n","\n","\\begin{bmatrix}\n","\\displaystyle\\sum_{i=1}^m x_i^4 & \\displaystyle\\sum_{i=1}^m x_i^3 & \\displaystyle\\sum_{i=1}^m x_i^2 \\\\\n","\\displaystyle\\sum_{i=1}^m x_i^3 & \\displaystyle\\sum_{i=1}^m x_i^2 & \\displaystyle\\sum_{i=1}^m x_i \\\\\n","\\displaystyle\\sum_{i=1}^m x_i^2 & \\displaystyle\\sum_{i=1}^m x_i & m \\\\\n","\\end{bmatrix}^{-1}\n","\n","\\begin{bmatrix}\n","\\displaystyle\\sum_{i=1}^m x_i^2y_i\\\\\n","\\displaystyle\\sum_{i=1}^m x_iy_i\\\\\n","\\displaystyle\\sum_{i=1}^m y_i\\\\\n","\\end{bmatrix}\n","\n","\\tag{5}\n","\n","\\end{aligned}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"kcWdWt8F68nR"},"source":["\n","\n","---\n"]},{"cell_type":"markdown","metadata":{"id":"ZAjNXSsJuPDn"},"source":["### **(b)** In this problem, we would like to use a linear regressor to fit the data, where $\\hat{y}(x)=ax+b$ with $a,b,x$ being scalars. Denote $\\beta_{LS} = \\begin{bmatrix} a \\\\ b \\end{bmatrix}$ to contain the regressor coefficients, and recall that the linear algebraic formula for least squares gives $\\beta_{LS} = (A^\\top A)^{-1} A^\\top y$ with $A^\\dagger=(A^\\top A)^{-1} A^\\top$ known as the pseudo-inverse of $A$. \n","\n","### In this problem, we ask you to \n","### **#1.** Use the function `np.linalg.pinv` to find the values of regressor coefficients $\\beta_{LS} $ and match it with your previous result. Note that the following piece of starter code generates a random least squares regression dataset with 500 data-points. \n","\n","\n","### **#2.** Further match your results by directly solving the problem using the builtin numpy function: `np.linalg.lstsq`\n","### **#3.** Plot a graph between $X$ and $y$, and overlay it with the linear regression line. (6 points)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":972,"status":"ok","timestamp":1666369517913,"user":{"displayName":"Mohammad Ali Khan","userId":"09268694697598189578"},"user_tz":420},"id":"odrt11XjuEb7","outputId":"e59fc3f3-b24b-4738-94e3-5eec2bcc1523"},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of X is: (500, 1)\n","Shape of y is: (500,)\n"]}],"source":["### !!! DO NOT EDIT !!!\n","# starter code to generate a random least squares regression dataset with 500 points\n","import numpy as np\n","from scipy import optimize\n","import matplotlib.pyplot as plt\n","from sklearn import datasets\n","\n","# generate x and y\n","X, y =  datasets.make_regression(n_samples=500, n_features=1, n_informative=1, n_targets=1, bias=10, noise=25, random_state=42, coef=False)\n","print('Shape of X is:', X.shape)\n","print('Shape of y is:', y.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RImfs6SU5bAs"},"outputs":[],"source":["#######\n","# !!! YOUR CODE HERE !!!\n","\n","#######"]},{"cell_type":"markdown","metadata":{"id":"fmPom4m97CH4"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"QjDICRJk7dW3"},"source":["### **(c)** In this problem, we ask you to \n","### **#1.** Write a function `my_func_fit (X,y)`, where `X` and `y` are column vectors of the same size containing experimental data. The function should return the values for $\\alpha$ and $\\beta$ which are the scalar parameters of the estimation function $ùë¶ÃÇ (ùë•)=ùõºùë•^ùõΩ$.\n","### **#2.** Test your code on the generated sample dataset and report the coefficients. The given piece of starter code generates a logarithmic dataset. \n","### **#3.** Plot a graph between $X$ vs $y$, and overlay it with the linear regression line. (8 points)\n","\n","**Linear regression for non-linear estimation function:** "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1666369517919,"user":{"displayName":"Mohammad Ali Khan","userId":"09268694697598189578"},"user_tz":420},"id":"Oua89wUM7Bqr","outputId":"61008b02-f1ac-4781-b4b1-d5e17ea963ea"},"outputs":[{"name":"stdout","output_type":"stream","text":["Shape of X is: (101,)\n","Shape of y is: (101,)\n"]}],"source":["### !!! DO NOT EDIT !!!\n","# starter code to generate a random exponential dataset\n","X = np.linspace(1, 10, 101)\n","y = 2*(X**(0.3)) + 0.3*np.random.random(len(X))\n","print('Shape of X is:', X.shape)\n","print('Shape of y is:', y.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xM_yXf_B8YRl"},"outputs":[],"source":["#######\n","# !!! YOUR CODE HERE !!!\n","\n","#######"]},{"cell_type":"markdown","metadata":{"id":"_Yrz5pPQ8yaY"},"source":["\n","\n","---\n","\n"]},{"cell_type":"markdown","metadata":{"id":"n6bBltHe9wM3"},"source":["### **(d)** In this problem, we ask you to \n","\n","### **#1.** Write a function `my_lin_regression(f, X, y)`, where `f` is a list containing function objects to basis functions that are pre-defined, and `X` and `y` are arrays containing noisy data. Assume that `X` and `y` are the same size, i.e, $X^{(i)}\\in \\mathbb{R}, y^{(i)}\\in \\mathbb{R}$. Return an array `beta` which represent the coefficients of the solved problem. I.e. we are solving the $\\beta$ which contains the coefficients in the regressor $ùë¶ÃÇ (ùë•)=ùõΩ_1‚ãÖùëì_1(x)+ùõΩ_2‚ãÖùëì_2(x)+‚ãØ+ùõΩ_n‚ãÖùëì_n(ùë•)$ with $f_i$ being basis functions. \n","\n","### **#2.** Also write a function `regression_plot(f,X,y,beta)` which plots a graph between `X` and `y`, and overlays it with the regression line. A few test scenarios are given to validate your code. (10 points)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kpqckqKv9t5p"},"outputs":[],"source":["#######\n","# !!! YOUR CODE HERE !!!\n","\n","#######"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DHIs_4zc8yEP"},"outputs":[],"source":["### !!! DO NOT EDIT !!!\n","### Test-1\n","X = np.linspace(0, 2*np.pi, 1000)\n","y = 3*np.sin(X) - 2*np.cos(X) + np.random.random(len(X))\n","f = [np.sin, np.cos] # f1 = sin, f2 = cos\n","\n","beta = my_lin_regression(f, X, y)\n","regression_plot(f,X,y,beta)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m06AxpFKCrCK"},"outputs":[],"source":["### !!! DO NOT EDIT !!!\n","### Test-2\n","X = np.linspace(0, 1, 1000)\n","y = 2*np.exp(0.5*X) + 0.25*np.random.random(len(X))\n","f = [np.exp] # f1 = exp\n","\n","beta = my_lin_regression(f, X, y)\n","regression_plot(f,X,y,beta)"]},{"cell_type":"markdown","metadata":{"id":"whHiUTe--qKn"},"source":["\n","\n","---\n","\n","\n","\n","---\n","\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"dsc210","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:43:44) [Clang 13.0.1 ]"},"vscode":{"interpreter":{"hash":"50f84938634b178e77a06dd2b7ac708810c1d9fc866924f1f275c7c6b9eb5965"}}},"nbformat":4,"nbformat_minor":0}
