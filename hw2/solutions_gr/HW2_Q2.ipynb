{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HW2_Q2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# HW2 - Q2: MNIST (35 points)\n","\n","**Keywords**: Multiclass Classification, Least Squares Regression, PyTorch\n","\n","**About the dataset**: \\\n","*   The [MNIST](https://en.wikipedia.org/wiki/MNIST_database) database (Modified National Institute of Standards and Technology database) is a large database of handwritten digits that is commonly used for training various image processing systems.\n","*   The MNIST database contains 70,000 labeled images. Each datapoint is a $28\\times 28$ pixels grayscale image.\n","*   However because of compute limitations, we will use a much smaller dataset with size $8\\times 8$ images. These images are loaded from `sklearn.datasets`.\n","\n","**Agenda**:\n","* In this programming challenge, you will be performing multiclass classification on the simplified MNIST dataset.\n","* You will be applying Multiclass Logistic Regression from scratch. You will work with both Mean Square Error (L2) loss and Cross Entropy (CE) loss with gradient descent (GD) as well as stochastic/mini-batch gradient descent (SGD). \n","* You will also see how using PyTorch does much of the heavylifting for modeling and training.\n","* Finally, you will train a 2-hidden-layer Neural Network model on the image dataset.\n","* All the predictions will be evaluated on a test set.\n","\n","**Note:**\n","* Hardware accelaration is not needed but is recommended!\n","* A note on working with GPU:\n","  * Take care that whenever declaring new tensors, set `device=device` in parameters. \n","  * You can also move a declared torch tensor/model to device using `.to(device)`. \n","  * To move a torch model/tensor to cpu, use `.to('cpu')`\n","  * Keep in mind that all the tensors/model involved in a computation have to be on the same device (CPU/GPU).\n","* Run all the cells in order.\n","* **Do not edit** the cells marked with !!DO NOT EDIT!!\n","* Only **add your code** to cells marked with !!!! YOUR CODE HERE !!!!\n","* Do not change variable names, and use the names which are suggested."],"metadata":{"id":"hjRM13EAuL3g"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"ZUJgYttwjmpK"}},{"cell_type":"code","source":["# !!DO NOT EDIT!!\n","# imports\n","import torch\n","import numpy as np\n","import math\n","from tqdm.notebook import tqdm\n","import matplotlib.pyplot as plt\n","from sklearn.datasets import load_digits\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","# loading the dataset directly from the scikit-learn library\n","dataset = load_digits()\n","X = dataset.data\n","y = dataset.target\n","print('Number of images:', X.shape[0])\n","print('Number of features per image:', X.shape[1])"],"metadata":{"id":"qNtGmOhl1I_m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !!DO NOT EDIT!!\n","# utility function to plot gallery of images\n","def plot_gallery(images, titles, height, width, n_row=2, n_col=4):\n","    plt.figure(figsize=(2* n_col, 3 * n_row))\n","    plt.subplots_adjust(bottom=0, left=0.01, right=0.99, top=0.90, hspace=0.35)\n","    for i in range(n_row * n_col):\n","        plt.subplot(n_row, n_col, i + 1)\n","        plt.imshow(images[i].reshape((height, width)), cmap=plt.cm.gray)\n","        plt.title(titles[i], size=12)\n","        plt.xticks(())\n","        plt.yticks(())\n","\n","# visualize some of the images of the MNIST dataset\n","plot_gallery(X, y, 8, 8, 2, 5)"],"metadata":{"id":"qDy8xSdXyJI3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !!DO NOT EDIT!!\n","# Let us split the dataset into training and test sets in a stratified manner.\n","# Note that we are not creating evaluation datset as we will not be tuning hyper-parameters\n","# The split ratio is 4:1\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n","print('Shape of train dataset:', X_train.shape)\n","print('Shape of evaluation dataset:', X_test.shape)"],"metadata":{"id":"lLRlr0wNOKI1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !!DO NOT EDIT!!\n","# define some constants - useful for later\n","num_classes = len(np.unique(y)) # number of target classes = 10 -- (0,1,2,3,4,5,6,7,8,9)\n","num_features = X.shape[1]       # number of features = 64\n","max_epochs = 100000             # max number of epochs for training\n","lr = 1e-2                       # learning rate\n","tolerance = 1e-6                # tolerance for early stopping during training"],"metadata":{"id":"T6Jam6RdFeIm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !!DO NOT EDIT!!\n","# Hardware Accelaration: to set device if using GPU.\n","# You can change runtime in colab by naviagting to (Runtime->Change runtime type), and selecting GPU in hardware accelarator.\n","# NOTE that you can run this homework without GPU.\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'"],"metadata":{"id":"hgMgGeeUUFb-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"BBvXcyFhmLvU"}},{"cell_type":"markdown","source":["### **(a)** In this section, we will apply multiclass logistic regression from scratch with one-vs-all strategy using gradient descent (GD) as well as stochastic gradient descent (SGD) with Mean Squared Error (MSE) loss. (8 points)\n","### We will be using a linear model $y^{(i)} = W \\mathbf{x}^{(i)},$ where $ W_{p\\times n}= \\begin{bmatrix}\n","\\leftarrow & \\mathbf{w}_1^\\top & \\rightarrow \\\\\n","\\leftarrow & \\mathbf{w}_2^\\top & \\rightarrow\\\\\n"," & \\vdots &  \\\\\n","\\leftarrow & \\mathbf{w}_p^\\top & \\rightarrow \\\\\n","\\end{bmatrix}$, and $p$ is the number of target classes. Also, $\\mathbf{x}^{(i)}\\in \\mathbb{R}^n, y^{(i)}\\in \\mathbb{R}$, and\n","### $X = \\begin{bmatrix}\n","\\uparrow &  \\uparrow & \\dotsm &  \\uparrow\\\\\n","\\mathbf{x}^{(1)} & \\mathbf{x}^{(2)} & \\dotsm & \\mathbf{x}^{(m)} \\\\\n","\\downarrow & \\downarrow  & \\dotsm & \\downarrow  \\\\\n","\\end{bmatrix}, Y = \\begin{bmatrix}\n","y^{(1)}\\\\\n","y^{(2)} \\\\\n","\\vdots \\\\\n","y^{(m)} \\\\\n","\\end{bmatrix}$, where $m$ is the number of datapoints.\n","\n","\n","### **#1.** Follow the steps outlined below:"],"metadata":{"id":"nhpUZN-ES_e0"}},{"cell_type":"code","source":["# 1. Scale the features between 0 and 1 \n","# To scale, you can directly use the MinMaxScaler from sklearn.\n","#######\n","# !!!! YOUR CODE HERE !!!!\n","\n","# output variable names -  X_train, X_test\n","#######"],"metadata":{"id":"cQGB7Y9cUO2V"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2. One-Hot encode the target labels\n","# To one-hot encode, you can use the OneHotEncoder from sklearn\n","#######\n","# !!!! YOUR CODE HERE !!!!\n","\n","# output variable names -  y_train_ohe, y_test_ohe\n","#######\n","print('Shape of y_train_ohe:',y_train_ohe.shape)\n","print('Shape of y_test_ohe:',y_test_ohe.shape)"],"metadata":{"id":"iZRF8HumUlen"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Note:** Here we need to define the model prediction. The input matrix is $X_{n\\times m}$ where $m$ is the number of examples, and $n$ is the number of features. The linear predictions can be given by: $Y = WX + b$ where $W$ is a $p\\times n$ weight matrix and $\\mathbf{b}$ is a $p$ size bias vector. $p$ is the number of target labels."],"metadata":{"id":"lJN3wY7CXe10"}},{"cell_type":"markdown","source":["### **#2.** Define a function `linear_model` that takes as input a weight matrix (`W`), bias vector (`b`), and input data matrix of size $m\\times n$ (`XT`). This function should return the predictions $\\hat{y}$. "],"metadata":{"id":"AztFFkzUuHW1"}},{"cell_type":"code","source":["#######\n","# !!!! YOUR CODE HERE !!!!\n","\n","#######"],"metadata":{"id":"75DOhP-EVsRs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Note:** The loss function that we would be using is the Mean Square Error (L2) Loss:\\\n","$\\displaystyle MSE = \\frac{1}{m}\\sum_{i=1}^m(\\hat{y}^{(i)}-y^{(i)})^2$, where $m$ is the number of examples, $\\hat{y}^{(i)}$ is the \n","predicted value and $y^{(i)}$ is the ground truth."],"metadata":{"id":"ejbSxyl2du2L"}},{"cell_type":"markdown","source":["### **#3.**Define a function `mse_loss` that takes as input prediction (`y_pred`) and actual labels (`y`), and returns the MSE loss."],"metadata":{"id":"8snSDdp4u49o"}},{"cell_type":"code","source":["#######\n","# !!!! YOUR CODE HERE !!!!\n","\n","#######"],"metadata":{"id":"MGjd_pYPduV0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the following part, we will do some setup required for training such as initializing weights and biases moving everything to torch tensors.\n","\n","### **#4.** Define a function: `initializeWeightsAndBiases` that returns tuple `(W, b)`, where `W` is a randomly generated torch tensor of size `num_classes x num_features`, and `b` is a randomly generated torch vector of size `num_classes`. For both the tensors, set `requires_grad=True` in parameters."],"metadata":{"id":"d27EoDd1vIA7"}},{"cell_type":"markdown","source":["### **#5.** Additionally, Move all training and testing data to torch tensors with `dtype=float32`. Remember to set `device=device` in parameters."],"metadata":{"id":"Rqj4Xy_QwRBs"}},{"cell_type":"code","source":["#######\n","# !!!! YOUR CODE HERE !!!!\n","\n","# output variable names -  X_train_torch, X_test_torch, y_train_ohe_torch, y_test_ohe_torch\n","#######"],"metadata":{"id":"B8YiUt1ewQYz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **#6.** In this part we will implement the code for training. Given below is a function: `train_linear_regression_model` that takes as input max number of epochs (`max_epochs`), batch size (`batch_size`), Weights (`W`), Biases (`b`), training data (`X_train, y_train`), learning rate (`lr`), tolerance for stopping (`tolerance`). It return a tuple `(W,b,losses)` where `W,b` are the trained weigths and biases respectively, and `losses` is a list of tuples of loss logged every $100^{th}$  epoch.\n","\n","Complete each of the steps outlines below. You can go through [this](https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e#ea0d) article for reference."],"metadata":{"id":"wpiEdcLYg6Vb"}},{"cell_type":"code","source":["# Define a function train_linear_regression_model\n","def train_linear_regression_model(max_epochs, batch_size, W, b, X_train, y_train, lr, tolerance):\n","  losses = []\n","  prev_loss = float('inf')\n","  number_of_batches = math.ceil(len(X_train)/batch_size)\n","\n","  for epoch in tqdm(range(max_epochs)):\n","    for i in range(number_of_batches):\n","      X_train_batch = X_train[i*batch_size: (i+1)*batch_size]\n","      y_train_batch = y_train[i*batch_size: (i+1)*batch_size]\n","     \n","      #######\n","      # !!!! YOUR CODE HERE !!!!\n","      # 7. do prediction\n","\n","      # 8. get the loss\n","      # loss = ______\n","\n","      # 9. backpropagate loss\n","\n","      # 10. update the weights and biasees\n","      \n","      # 11. set the gradients to zero\n","      \n","      #######\n","\n","    # log loss every 100th epoch and print every 5000th epoch:\n","    if epoch%100==0:\n","      losses.append((epoch, loss.item()))\n","      if epoch%5000==0:\n","        print('Epoch: {}, Loss: {}'.format(epoch, loss.item()))\n","\n","    # break if decrease in loss is less than threshold\n","    if abs(prev_loss-loss)<=tolerance:\n","      break\n","    else:\n","      prev_loss=loss  \n","\n","  # return updated weights, biases, and logged losses\n","  return W, b, losses"],"metadata":{"id":"xZrxj7NEJhiw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **#7.** Initialize weights and biases using the `initializeWeightsAndBiases` function that you defined earlier, and train your model using function `train_linear_regression_model` defined above. Use full batch (set `batch_size=len(X_train)` for training (Gradient Descent). Also plot the graph of loss vs number of epochs (Recall that values for learning rate (`lr`) and tolerance (`tolerance`) are already defined above)."],"metadata":{"id":"QFJ-tBGgeOdw"}},{"cell_type":"code","source":["#######\n","# !!!! YOUR CODE HERE !!!!\n","\n","#######"],"metadata":{"id":"zHfBioX0K294"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !!DO NOT EDIT!!\n","# print accuracies of model\n","predictions_train = linear_model(W,b,X_train_torch).to('cpu')\n","predictions_test = linear_model(W,b,X_test_torch).to('cpu')\n","y_train_pred = torch.argmax(predictions_train, dim=1).numpy()\n","\n","y_test_pred = torch.argmax(predictions_test, dim=1).numpy()\n","print(\"Train accuracy:\",accuracy_score(y_train_pred, np.asarray(y_train, dtype=np.float32)))\n","print(\"Test accuracy:\",accuracy_score(y_test_pred, np.asarray(y_test, dtype=np.float32)))"],"metadata":{"id":"QIDhQXpTYX9T"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **#8.** Now, retrain the above model with `batch_size=64` (Stochastic/Mini-batch Gradient Descent) keeping else everything same. Like before, plot the graph between loss and number of epochs."],"metadata":{"id":"RZprgWslioL7"}},{"cell_type":"code","source":["#######\n","# !!!! YOUR CODE HERE !!!!\n","\n","#######"],"metadata":{"id":"5NMg8pDYinSo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !!DO NOT EDIT!!\n","# print accuracies of model\n","predictions_train = linear_model(W,b,X_train_torch).to('cpu')\n","predictions_test = linear_model(W,b,X_test_torch).to('cpu')\n","y_train_pred = torch.argmax(predictions_train, dim=1).numpy()\n","\n","y_test_pred = torch.argmax(predictions_test, dim=1).numpy()\n","print(\"Train accuracy:\",accuracy_score(y_train_pred, np.asarray(y_train, dtype=np.float32)))\n","print(\"Test accuracy:\",accuracy_score(y_test_pred, np.asarray(y_test, dtype=np.float32)))"],"metadata":{"id":"y5O3T2zkjy2S"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"QHPAmI5BmHkv"}},{"cell_type":"markdown","source":["### **(b)** In the previous question, we defined the model, loss, and even the gradient update step. We also had to manully set the grad to zero. In this question, we will re-implement the linear model and see how we can directly use Pytorch to do all this for us in a few simple steps. (6 points)"],"metadata":{"id":"iz8cNUXR5DOu"}},{"cell_type":"code","source":["# !! DO NOT EDIT !!\n","# common utility function to print accuracies\n","def print_accuracies_torch(model, X_train_torch, X_test_torch, y_train, y_test):\n","  predictions_train = model(X_train_torch).to('cpu')\n","  predictions_test = model(X_test_torch).to('cpu')\n","  y_train_pred = torch.argmax(predictions_train, dim=1).numpy()\n","  y_test_pred = torch.argmax(predictions_test, dim=1).numpy()\n","  print(\"Train accuracy:\",accuracy_score(y_train_pred, np.asarray(y_train, dtype=np.float32)))\n","  print(\"Test accuracy:\",accuracy_score(y_test_pred, np.asarray(y_test, dtype=np.float32)))"],"metadata":{"id":"Lwm9t0K0lbvl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **#1.** Define the linear model using PyTorch"],"metadata":{"id":"MSjn0TlT0_X5"}},{"cell_type":"code","source":["#######\n","# !!!! YOUR CODE HERE !!!!\n","# Define a model class using torch.nn\n","class Linear_Model(torch.nn.Module):\n","  def __init__(self):\n","    super(Linear_Model, self).__init__()\n","    # Initalize various layers of model as instructed below\n","    # 1. initialze one linear layer: num_features -> num_targets\n","\n","\n","  def forward(self, X):\n","    # 2. define the feedforward algorithm of the model and return the final output\n","\n","#######"],"metadata":{"id":"tAVqYskI4ohI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **#2.** In this part we will implement a general function for training a PyTorch model. Define a general training function: `train_torch_model` that takes as input an initialized torch model (`model`), batch size (`batch_size`), initialized loss (`criterion`), max number of epochs (`max_epochs`), training data (`X_train, y_train`), learning rate (`lr`), tolerance for stopping (`tolerance`). This function will return a tuple `(model, losses)`, where `model` is the trained model, and `losses` is a list of tuples of loss logged every $100^{th}$ epoch. Complete each of the steps outlines below. You can go through [this](https://towardsdatascience.com/understanding-pytorch-with-an-example-a-step-by-step-tutorial-81fc5f8c4e8e#ea0d) article for reference.  You can also refer Q3-(d) from HW1."],"metadata":{"id":"9zyx8lVTFq8q"}},{"cell_type":"code","source":["# Define a function train_torch_model\n","def train_torch_model(model, batch_size, criterion, max_epochs, X_train, y_train, lr, tolerance):\n","  losses = []\n","  prev_loss = float('inf')\n","  number_of_batches = math.ceil(len(X_train)/batch_size)\n","  \n","  #######\n","  # !!!! YOUR CODE HERE !!!!\n","  # 3. move model to device\n","\n","  # 4. define optimizer (use torch.optim.SGD (Stochastic Gradient Descent)) \n","  # Set learning rate to lr and also set model parameters \n","\n","  for epoch in tqdm(range(max_epochs)):\n","    for i in range(number_of_batches):\n","      X_train_batch = X_train[i*batch_size: (i+1)*batch_size]\n","      y_train_batch = y_train[i*batch_size: (i+1)*batch_size]\n","\n","      # 5. reset gradients\n","\n","      # 6. prediction\n","\n","      # 7. calculate loss\n","\n","      # 8. backpropagate loss\n","\n","      # 9. perform a single gradient update step\n","\n","  #######\n","\n","    # log loss every 100th epoch and print every 5000th epoch:\n","    if epoch%100==0:\n","      losses.append((epoch, loss.item()))\n","      if epoch%5000==0:\n","        print('Epoch: {}, Loss: {}'.format(epoch, loss.item()))\n","    \n","    # break if decrease in loss is less than threshold\n","    if abs(prev_loss-loss)<=tolerance:\n","      break\n","    else:\n","      prev_loss=loss  \n","\n","  # return updated model and logged losses\n","  return model, losses"],"metadata":{"id":"4T04zQPE53rS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **#3.** Initialize your model and loss function. Use `nn.MSELoss`. Use full batch for training (Gradient Descent). Also plot the graph of loss vs number of epochs."],"metadata":{"id":"Y7RzDMajj8sc"}},{"cell_type":"code","source":["#######\n","# !!!! YOUR CODE HERE !!!!\n","\n","#######"],"metadata":{"id":"LDm0__pE6edF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !!DO NOT EDIT!!\n","# print accuracies of model\n","print_accuracies_torch(model, X_train_torch, X_test_torch, y_train, y_test)"],"metadata":{"id":"TIMHwStJ8zzl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **#4.** Now, retrain the above model with `batch_size=64` (Stochastic/Mini-batch Gradient Descent) keeping else everything same. Like before, plot the graph between loss and number of epochs."],"metadata":{"id":"eMXOadCpkufn"}},{"cell_type":"code","source":["#######\n","# !!!! YOUR CODE HERE !!!!\n","\n","#######"],"metadata":{"id":"xsj_apLOkt17"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !!DO NOT EDIT!!\n","# print accuracies of model\n","print_accuracies_torch(model, X_train_torch, X_test_torch, y_train, y_test)"],"metadata":{"id":"cJHvwtH6k0F6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"nYiv8O3emFgn"}},{"cell_type":"markdown","source":["### **(c)** Now, instead of using MSELoss, we will use a much more natural loss function for logistic regression task which is the Cross Entropy Loss. (8 points)\n","**Note:**\n","The [Cross Entropy Loss](https://ebookreading.net/view/book/EB9781789130331_73.html) for multiclass calssification is the mean of the negative log likelihood of the output logits after softmax:\\\n","$L = \\underbrace{\\frac{1}{m} \\sum_{i=1}^m \\underbrace{-y^{(i)} \\underbrace{log \\underbrace{\\frac{e^{\\hat{y}^{(i)}}}{\\sum_{j=1}^p e^{\\hat{y}^{(j)}}}}_{\\text{Softmax}}}_{\\text{LogSoftmax}}}_{\\text{Negative Log Likelihood (NLL)}}}_{\\text{Cross Entropy (CE) Loss }}$,\n","\n","\n","where $y^{(i)}$ is the ground truth, and $\\hat{y}^{(k)}$ (also called as *logits*) represent the outputs of the last linear layer of the model. "],"metadata":{"id":"QMXvLv81HPCM"}},{"cell_type":"markdown","source":["### **#1.** Instead of `nn.MSELoss`, train the above model with `nn.CrossEntropyLoss`. Use full-batch. Also plot the graph between loss and number of epochs."],"metadata":{"id":"WQw1h-A3lCPp"}},{"cell_type":"code","source":["#######\n","# !!!! YOUR CODE HERE !!!!\n","\n","#######"],"metadata":{"id":"VXmF1XD_9Irh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !!DO NOT EDIT!!\n","# print accuracies of model\n","print_accuracies_torch(model, X_train_torch, X_test_torch, y_train, y_test)"],"metadata":{"id":"gOvC4HdUINQt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **#2.** Perform the same task above with `batch_size=64`. Also plot the graph of loss vs epochs."],"metadata":{"id":"KGQ_G6tHl8XP"}},{"cell_type":"code","source":["#######\n","# !!!! YOUR CODE HERE !!!!\n","\n","#######"],"metadata":{"id":"6fNrbHH6IOiz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !!DO NOT EDIT!!\n","# print accuracies of model\n","print_accuracies_torch(model, X_train_torch, X_test_torch, y_train, y_test)"],"metadata":{"id":"0o04L-LNmCkp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"2HgoKWSV73xk"}},{"cell_type":"markdown","source":["### **(d)** Now, we will train a neural network in pytorch with two hidden layers of sizes 32 and 16 neurons. We will use non-linear ReLU activations thus effectively making this a non-linear model. We will use this neural network model for multi-class classification with Cross Entropy Loss. (8 points)\n","\n","**Note:** The neural network model output can be represented mathematically as below:\\\n","$\\hat{y}^{(i)}_{10\\times1} = W^{(3)}_{10\\times 16}\\sigma(W^{(2)}_{16\\times 32}\\sigma(W^{(1)}_{32\\times 64}\\mathbf{x}^{(i)}_{64\\times1}+\\mathbf{b}^{(1)}_{32\\times1})+\\mathbf{b}^{(2)}_{16\\times1})+\\mathbf{b}^{(3)}_{10\\times1}$, \\\n","where $\\sigma$ represents ReLU activation, $W^{(i)}$ is the weight of the $i^{th}$ linear layer, and $\\mathbf{b}^{(i)}$ is the layer's bias. We use the subscript to denote the dimension for clarity. "],"metadata":{"id":"flKPhgoccRcL"}},{"cell_type":"markdown","source":["### **#1.** Define the 2-hidden layer neural network model below."],"metadata":{"id":"qXYnz6-eI8MG"}},{"cell_type":"code","source":["#######\n","# !!!! YOUR CODE HERE !!!!\n","# Define a neural network model class using torch.nn\n","class NN_Model(torch.nn.Module):\n","  def __init__(self):\n","    super(NN_Model, self).__init__()\n","    # Initalize various layers of model as instructed below\n","    # 1. initialize three linear layers: num_features -> 32, 32 -> 16, 16 -> num_targets\n","\n","    # 2. initialize RELU\n","\n","  def forward(self, X):\n","    # 3. define the feedforward algorithm of the model and return the final output\n","    # Apply non-linear ReLU activation between subsequent layers\n","\n","#######  "],"metadata":{"id":"d3PSTz2jcQ0j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **#2.** Train the newly defined Neural Network two hidden layer model with Cross Entropy Loss. Use full-batch and plot the graph of loss vs number of epochs. Note that you can re-use the training function `train_torch_model` (from part (b))."],"metadata":{"id":"mERRi4Eamze0"}},{"cell_type":"code","source":["#######\n","# !!!! YOUR CODE HERE !!!!\n","\n","#######"],"metadata":{"id":"pliWaICdmzEB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !!DO NOT EDIT!!\n","# print accuracies of model\n","print_accuracies_torch(model, X_train_torch, X_test_torch, y_train, y_test)"],"metadata":{"id":"EoRJpn8QnZrX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### **#3.** Re-train the above model with `batch_size=64`. Also plot the graph of loss vs epochs."],"metadata":{"id":"TP-JX4IznLQL"}},{"cell_type":"code","source":["#######\n","# !!!! YOUR CODE HERE !!!!\n","\n","#######"],"metadata":{"id":"Sx8e5dvJnK1n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# !!DO NOT EDIT!!\n","# print accuracies of model\n","print_accuracies_torch(model, X_train_torch, X_test_torch, y_train, y_test)"],"metadata":{"id":"mr7M3xyknaLp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","---\n","\n"],"metadata":{"id":"szUeNm2kotqY"}},{"cell_type":"markdown","source":["### **(e)**  In the above few problems, you performed several experiments with different batch size and loss functions. Write down an analysis of your observations from the results. (5 points)\n","Some points that you could cover are:\n","* Effect of using full vs. batch gradient descent.\n","* Effect of different loss strategy on performance.\n","* Effect of using linear vs. non-linear models.\n","* Training time per epoch in different cases.\n","\n","Also, plot a line graph of accuracy vs. model for both train and test sets. Recall that you trained the following models in this question:\n","\n","1.   Linear Model - Scratch + MSE Loss + Full Batch \n","2.   Linear Model - Scratch + MSE Loss + Mini Batch \n","3.   Linear Model - PyTorch + MSE Loss + Full Batch \n","4.   Linear Model - PyTorch + MSE Loss + Mini Batch \n","5.   Linear Model - PyTorch + CE Loss + Full Batch \n","6.   Linear Model - PyTorch + CE Loss + Mini Batch \n","7.   NN Model - PyTorch + CE Loss + Full Batch \n","8.   NN Model - PyTorch + CE Loss + Mini Batch \n"],"metadata":{"id":"RJ1HXvh_ovQ_"}},{"cell_type":"markdown","source":["#### <font color=\"red\">Your answer here:</font>"],"metadata":{"id":"k6e4sulhvLp5"}},{"cell_type":"markdown","source":["\n","\n","---\n","\n","\n","\n","---\n","\n"],"metadata":{"id":"obE47rGOvMo-"}}]}